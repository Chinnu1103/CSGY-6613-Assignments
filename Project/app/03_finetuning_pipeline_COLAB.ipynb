{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Milestone\n",
    "\n",
    "Note: This notebook was run on COLAB due to high GPU requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PoPKQjga6obN",
    "outputId": "7ea9a5d6-5812-43de-e2e1-04b672f7ed68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mΕ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Ε Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --progress-bar off\n",
    "from torch import __version__; from packaging.version import Version as V\n",
    "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "!pip install -qqq --no-deps {xformers} trl peft accelerate bitsandbytes triton --progress-bar off\n",
    "\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "matKaF-f-GiU"
   },
   "source": [
    "## 1. Load model for PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340,
     "referenced_widgets": [
      "3d10a773e6bb4a0da8f14d7e4016b32a",
      "11a683b037ff4310b7cc1bf7d0e9e3e2",
      "8825a0ca10654ddeaf7c58c1544953a0",
      "0561f19d2fbe4aef801c8fb536f41b9f",
      "caa394df371847cebf497fa030bb27b2",
      "bc044cdc0e6b4e61b8437b3d08b4c09e",
      "8adf1a9e656a4a69876eff77d0f3f851",
      "b514bece60b14840a668295a5ea0fc80",
      "697c34160273411490d799c21fc08317",
      "54380f27cf3444e99f5d2c741ce41b92",
      "2b1357a7590649cf94c90c12eb6e60be",
      "603730ade2eb4dbf8903e3ab10ad9cd8",
      "e82bb6497bdd41bd8b54628d1ee87c10",
      "e8cd6d9e2ce64c69baa7cee8eda03698",
      "9fdf5ef1792a490f841d24ec849f4cfe",
      "04f443a12db34e3abb07bac61e6e0391",
      "694d8c0c8fa24660949a2d0029127c29",
      "b11ad008e6a64a4c8bff36fc0eb8c456",
      "45c2c5f77397453a9f5c5cd99af303e0",
      "21dc5bf8443447e9a6e7952fada7a347",
      "cd8992c81c294beea484f514269ae00e",
      "25a50ae9d35a4e5d92a096b000a96ea3",
      "0957db7f37784779b5d186efc1f6eaad",
      "89a2149e2598483094e43bf20acc706a",
      "5a90bb8ab2174aefbf9de8c303f60d6c",
      "c54191c987d8434a87c31e06287727f4",
      "bfccf8c5d0d94c78bb44fca4130123b3",
      "1becbf72632d483b83241b72d36d2b36",
      "9956572a4a004623865f3965588dd1c4",
      "59b1e8ad643247c5850a16a65ba93a02",
      "d67b0928681947f98a27b68898cdcd56",
      "45e034b70899436d9bdaca03d97ee056",
      "80658efd6e324970b3db5477690bf5f0",
      "c5f7635b2bc44957ab47b6e703d4ad89",
      "3fd3fe6709914b56a0bd8e0cdfd5e329",
      "dace7d2aff8144778f7ec38669dfbe61",
      "ddee132a06514e8982b53f6ad18bfff6",
      "f4346b679d4e443f976552cdb875e1dd",
      "b17e144fd98b40c298725c06d7709642",
      "c2ba5a3233e54b4daf095727468486e5",
      "57c4b69a3d074583baeeaa0dcbd9e4a7",
      "431c91f76bdb4d30b66617d106d61cf2",
      "2e0776c5c8fa43a3be7dd7e91ec1c185",
      "4cdbe71cbc744df9b041c4b23002257e",
      "8881cfd89b7f49ba89e13b9c3da2a309",
      "187112d231374cb49bc7a399f7145985",
      "11258a344fa041de8b8c522956ff8ed6",
      "cbe34cb9b7c7464fa2649ed8e2521164",
      "f45356bc44464bc78f29bb1dd387dd3c",
      "04d850bc165f430f91da2ffc32f6db74",
      "f9f8068a2fbf42b089d35443494e87ae",
      "9b9348e5c677446dbdc5fc2921049b85",
      "e138b17d46e94b26846a37db4903e01d",
      "b7dd0760ff5441e28d3c8268df06d602",
      "4b9e0ea2499a4d55b27675d0b5834136"
     ]
    },
    "id": "zGX9wG7Lhc-z",
    "outputId": "a79c06d5-cfd8-418a-efa1-68a4c3ad3be2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.2: Fast Llama patching. Transformers:4.46.2.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d10a773e6bb4a0da8f14d7e4016b32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603730ade2eb4dbf8903e3ab10ad9cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0957db7f37784779b5d186efc1f6eaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f7635b2bc44957ab47b6e703d4ad89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8881cfd89b7f49ba89e13b9c3da2a309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-3B\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "# Prepare model for PEFT\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\"\n",
    ")\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjDpwfjJ3RAL"
   },
   "source": [
    "## 2. Prepare data and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "8514de2b5fb64f8b89408d594516c4f6",
      "20b91636b855448ab2d5803a7e447b37",
      "7091c107535344468537ab53ba4aee78",
      "e0329fc5e2104d8295aa2d11d0ef5c41",
      "2090dbc4252947ef8d0ceacf98ff2062",
      "6a7aef730a0e4487bc15a41e9fa2e838",
      "be7bc22e35674924b2e416042ffd03f5",
      "cc0278626d8b4b748572a7d43ac0aa0f",
      "7524f4ee46de444c8a2f363a59bfa375",
      "be3207f979254634b612b26f53b095d5",
      "975b12029a3d45ce9aff069dc42a03a0",
      "35752176c09147359968d9e065966775",
      "8cf4811b5b2048eeaed6daf7f1a47764",
      "6460b8ff42d34bf992eb66da8049bf35",
      "089f004dd0044d6d816705ad0bdb0a0f",
      "7889adbe472e46268e9d56d6e4692ff6",
      "7587b5e7516448e6a9fa6d97be0159ad",
      "61c9d92a11744ee0aa1bfa201d5897f6",
      "68446edbfdcb4b2cb3ecfbf57bddd270",
      "be50c29e17a34da2b3b5fbbc63a84cf1",
      "cc0d751538294908ab6aac0e2d744f2f",
      "3233b05efc58446a9c05f3723ad06e19"
     ]
    },
    "id": "sqGnvaT8is-R",
    "outputId": "1c8a873a-47a3-453c-e2de-459e68adf058"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8514de2b5fb64f8b89408d594516c4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35752176c09147359968d9e065966775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}\n",
    ")\n",
    "\n",
    "def apply_template(examples):\n",
    "    messages = examples[\"conversations\"]\n",
    "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = Dataset.from_json(\"formatted_dataset.json\")\n",
    "dataset = dataset.map(apply_template, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdfjufQd3XMi"
   },
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "280c1fdbb9e544df8ab776329b18fca9",
      "78de5339cdd244b5b803c3fcc0cb19fd",
      "0f182d0722a8492f9052c0b96dee1a67",
      "63bdcfb65cde463a9e57c0d5dbb764d2",
      "871b04f4b63b479d9cbc337668493146",
      "0f4a16d145d94095a8ae81c8d09e9c04",
      "2594998316c248c59701868949591fca",
      "c441f839175849dc9e5053817b04ec38",
      "4641a1357c9e413da26a8ab39599708e",
      "d4c2aac0f0d04bda81482db0c0869d06",
      "34a85108be5644dcb2ea0c861c1b69c5"
     ]
    },
    "id": "gcPAQihcjcfl",
    "outputId": "64b073af-9190-443c-e410-362ac9291e5e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280c1fdbb9e544df8ab776329b18fca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 16 | Total steps = 31\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 路路路路路路路路路路\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241204_152657-x41z7zp4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chirangivibhat-new-york-university/huggingface/runs/x41z7zp4' target=\"_blank\">output</a></strong> to <a href='https://wandb.ai/chirangivibhat-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chirangivibhat-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/chirangivibhat-new-york-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chirangivibhat-new-york-university/huggingface/runs/x41z7zp4' target=\"_blank\">https://wandb.ai/chirangivibhat-new-york-university/huggingface/runs/x41z7zp4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 23:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.866800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.760400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.888600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.671400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.707400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.800700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.630400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.574700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.432200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.534800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.439000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.516000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.437200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.440800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.429900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.334600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.313200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.338500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.351600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31, training_loss=1.5075826952534337, metrics={'train_runtime': 1537.5821, 'train_samples_per_second': 0.325, 'train_steps_per_second': 0.02, 'total_flos': 1.7328026161250304e+16, 'train_loss': 1.5075826952534337, 'epoch': 0.992})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,\n",
    "    args=TrainingArguments(\n",
    "        learning_rate=3e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=10,\n",
    "        output_dir=\"output\",\n",
    "        seed=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI_U9FHZ3ZLO"
   },
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JXdjsLqkvZY",
    "outputId": "21528a1c-d686-4aca-9693-2dc96a186960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "This is the document context:\n",
      "Context 1 Title: ros-navigation/docs.nav2.org\n",
      "Context content: \n",
      "## Costmap2D `current_` Usage露\n",
      "\n",
      "In costmap2D, `current_` was used in ROS1 to represent whether a costmap layer\n",
      "was still enabled and actively processing data. It would be turned to `false`\n",
      "only under the situation that the expected update rate of a sensor was not\n",
      "met, so it was getting stale or no messages. It acts as a fail-safe for if a\n",
      "navigation sensor stops publishing.\n",
      "\n",
      "In galactic, that will remain turn, however it will also add additional\n",
      "capabilities. It is also now set to `false` when a costmap is reset due to\n",
      "clearing or other navigation recoveries. That stops the robot from creating a\n",
      "plan or control effort until after the costmap has been updated at least once\n",
      "after a reset. This enables us to make sure we cannot ever create a path or\n",
      "control with a completely empty costmap, potentially leading to collisions,\n",
      "due to clearing the costmap and then immediately requesting an algorithm to\n",
      "run.\n",
      "\n",
      "## Standard time units in parameters露\n",
      "\n",
      "\n",
      "Context 2 Title: ros-navigation/docs.nav2.org\n",
      "Context content: \n",
      "Architecturally, costmap filters consists from `CostmapFilter` class which is\n",
      "a basic class incorporating much common of its inherited filter plugins:\n",
      "\n",
      "  * `KeepoutFilter`: keep-out/safety zones filter plugin.\n",
      "\n",
      "  * `SpeedFilter`: slow/speed-restricted areas filter.\n",
      "\n",
      "  * Preferred lanes in industries. This plugin is covered by `KeepoutFilter` (see discussion in corresponding PR for more details).\n",
      "\n",
      "Each costmap filter subscribes to filter info topic (publishing by Costmap\n",
      "Filter Info Publisher Server) having all necessary information for loaded\n",
      "costmap filter and filter mask topic. `SpeedFilter` additionally publishes\n",
      "maximum speed restricting messages targeted for a Controller to enforce robot\n",
      "wont exceed given limit.\n",
      "\n",
      "\n",
      "Context 3 Title: ros-navigation/docs.nav2.org\n",
      "Context content: \n",
      "Several example implementations are included in `nav2_waypoint_follower`.\n",
      "`WaitAtWaypoint` and `PhotoAtWaypoint` plusings are included in\n",
      "`nav2_waypoint_follower` as run-time loadable plugins. `WaitAtWaypoint` simply\n",
      "lets robot to pause for a specified amount of time in milliseconds, at\n",
      "waypoint arrivals. While `PhotoAtWaypoint` takes photos at waypoint arrivals\n",
      "and saves the taken photos to specified directory, the format for taken photos\n",
      "also can be configured through parameters. All major image formats such as\n",
      "`png`, `jpeg`, `jpg` etc. are supported, the default format is `png`.\n",
      "\n",
      "Loading a plugin of this type is done through\n",
      "`nav2_bringup/params/nav2_param.yaml`, by specifying plugins name, type and\n",
      "its used parameters.\n",
      "\n",
      "\n",
      "This is the user prompt:  What is the role of the `current_` variable in the Costmap2D class in ROS2 and how has its functionality changed in the Galactic release?\n",
      " Can you describe the structure and purpose of costmap filters in ROS2's navigation stack?\n",
      " How can plugins like `WaitAtWaypoint` and `PhotoAtWaypoint` be utilized in the `nav2_waypoint_follower` in ROS2, and what parameters are essential for their configuration?\n",
      "\n",
      "\n",
      "Using the given document context pertaining to ROS2, answer the question prompted by the user.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "In ROS2, the `current_` variable in the Costmap2D class serves as a flag indicating whether a costmap layer is actively processing data. In the previous release, ROS1, this variable was set to `false` when the expected update rate of a sensor was not met, which could lead to stale data. However, in the Galactic release, this behavior remains unchanged, but it also introduces additional capabilities.\n",
      "When a costmap is reset due to clearing or other navigation recoveries, the `current_` variable is set to `false`, which prevents the robot from creating a plan or control effort until the costmap has been updated at least once after the reset. This ensures that the robot does not attempt to navigate with a completely empty costmap, which could potentially lead to collisions.\n",
      "The costmap filters in ROS2 are structured around the `CostmapFilter` class, which is a basic class that incorporates common functionality for various filter plugins. These plugins include:\n",
      "1. **KeepoutFilter**: This filter is responsible for defining safety zones around the robot.\n",
      "2. **SpeedFilter**: This filter restricts the robot's speed in specific areas.\n",
      "3. **Preferred Lanes in Industries**: This filter is covered by the `KeepoutFilter` and is used in industrial environments to ensure that the robot follows preferred lanes.\n",
      "Each filter subscribes to two topics:\n",
      "   - `filter_info`: This topic publishes information about the loaded costmap filter.\n",
      "   - `filter_mask`: This topic publishes a mask indicating which areas are filtered.\n",
      "Additionally, the `SpeedFilter` also publishes messages to the controller to enforce speed restrictions.\n",
      "Several example plugins are included in the `nav2_waypoint_follower`, such as `WaitAtWaypoint` and `PhotoAtWaypoint`. The `WaitAtWaypoint` simply allows the robot to pause for a specified amount of time at waypoint arrivals, while `PhotoAtWaypoint` takes photos at waypoint arrivals and saves them to a specified directory. The format for these photos can be configured through parameters, allowing for various image formats such as `png`, `jpeg`, and `jpg`.\n",
      "To load these plugins, developers can specify their names, types, and parameters in the `nav2_bringup/params/nav2_param.yaml` file. This configuration allows for dynamic management of plugins during runtime.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Load model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"This is the document context:\\nContext 1 Title: ros-navigation/docs.nav2.org\\nContext content: \\n## Costmap2D `current_` Usage\\u00b6\\n\\nIn costmap2D, `current_` was used in ROS1 to represent whether a costmap layer\\nwas still enabled and actively processing data. It would be turned to `false`\\nonly under the situation that the expected update rate of a sensor was not\\nmet, so it was getting stale or no messages. It acts as a fail-safe for if a\\nnavigation sensor stops publishing.\\n\\nIn galactic, that will remain turn, however it will also add additional\\ncapabilities. It is also now set to `false` when a costmap is reset due to\\nclearing or other navigation recoveries. That stops the robot from creating a\\nplan or control effort until after the costmap has been updated at least once\\nafter a reset. This enables us to make sure we cannot ever create a path or\\ncontrol with a completely empty costmap, potentially leading to collisions,\\ndue to clearing the costmap and then immediately requesting an algorithm to\\nrun.\\n\\n## Standard time units in parameters\\u00b6\\n\\n\\nContext 2 Title: ros-navigation/docs.nav2.org\\nContext content: \\nArchitecturally, costmap filters consists from `CostmapFilter` class which is\\na basic class incorporating much common of its inherited filter plugins:\\n\\n  * `KeepoutFilter`: keep-out/safety zones filter plugin.\\n\\n  * `SpeedFilter`: slow/speed-restricted areas filter.\\n\\n  * Preferred lanes in industries. This plugin is covered by `KeepoutFilter` (see discussion in corresponding PR for more details).\\n\\nEach costmap filter subscribes to filter info topic (publishing by Costmap\\nFilter Info Publisher Server) having all necessary information for loaded\\ncostmap filter and filter mask topic. `SpeedFilter` additionally publishes\\nmaximum speed restricting messages targeted for a Controller to enforce robot\\nwon\\u2019t exceed given limit.\\n\\n\\nContext 3 Title: ros-navigation/docs.nav2.org\\nContext content: \\nSeveral example implementations are included in `nav2_waypoint_follower`.\\n`WaitAtWaypoint` and `PhotoAtWaypoint` plusings are included in\\n`nav2_waypoint_follower` as run-time loadable plugins. `WaitAtWaypoint` simply\\nlets robot to pause for a specified amount of time in milliseconds, at\\nwaypoint arrivals. While `PhotoAtWaypoint` takes photos at waypoint arrivals\\nand saves the taken photos to specified directory, the format for taken photos\\nalso can be configured through parameters. All major image formats such as\\n`png`, `jpeg`, `jpg` etc. are supported, the default format is `png`.\\n\\nLoading a plugin of this type is done through\\n`nav2_bringup/params/nav2_param.yaml`, by specifying plugin\\u2019s name, type and\\nit\\u2019s used parameters.\\n\\n\\nThis is the user prompt:  What is the role of the `current_` variable in the Costmap2D class in ROS2 and how has its functionality changed in the Galactic release?\\n Can you describe the structure and purpose of costmap filters in ROS2's navigation stack?\\n How can plugins like `WaitAtWaypoint` and `PhotoAtWaypoint` be utilized in the `nav2_waypoint_follower` in ROS2, and what parameters are essential for their configuration?\\n\\n\\nUsing the given document context pertaining to ROS2, answer the question prompted by the user.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=1000, use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HunPZjPp3aWe"
   },
   "source": [
    "## 5. Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLqnTfDVYUF3",
    "outputId": "367f86cc-e6ab-4353-de47-02954bc8a1ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: write).\n",
      "The token `Colab_Write` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `Colab_Write`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649,
     "referenced_widgets": [
      "4521fc777b084c0d949b9333a09b4ef0",
      "feed91a06a1143648e570b35392b1858",
      "fe2b3f93035b4fb58c355721ff2faaa3",
      "790377199f3544c2856225c388123bbf",
      "fba5a1acc6564da7b539379a01bb7eb2",
      "cba9738b4096450988f4d3cf22b4a0de",
      "dfb2ba18082446f09b7bdc237dc6aa25",
      "35887fa1ec4e4e0b92f918c803ce48ac",
      "4729a9af410b43ceacc62fab0b4ac300",
      "cda1c002fc084bfc9b0b9d42f53cd251",
      "a79742df9853489c99d8230e37957718",
      "4844464e75c64d539282a9c5e18bb5e2",
      "aec675d6c87b429e8944d652519f7b55",
      "06e8bc0345bf42ab90c7df239359a5c2",
      "ded49622d0644be9ab266b777c109975",
      "525d4c171a304224a8826afbdbe5e765",
      "66370192cdcb4d288fc08dfc6c1bb164",
      "4b76d52495204447aa80c9faf28dfb5b",
      "9ea5cd71a6684bb897a41e748e39103b",
      "cdf5969d7f584dabb34bd41937319c1e",
      "0b588b219e9c464f9bd7a420e169d900",
      "86de80a1bb9343319b9a199f38c2ee8c",
      "3e15242e72bb459cad9651afdfd7c538",
      "41cb3d35829b47318fa293480dc22b2e",
      "00c0e279dbfe4f7c8cd83b81a59ddd55",
      "9c1fee69dd10400095cf06324c86cb6c",
      "3c3c328d9ee6426b9109a3e324d488c3",
      "359831ded9074dbc82084504ab66d5a8",
      "20b668f5021a4522b8966afab2e52702",
      "a3b36a7460fe42809f54c6fb08d0a6d7",
      "64aa5c3796454cb7a1877a8855bf87e5",
      "18d82fbd47d14d94a9a3f7676681e76c",
      "95df967616fa485caee4966bea81d112",
      "b7a477c801904d788ab851e8933c4a7f",
      "ab30c2a8c3834a8eb175b3c1332f42d5",
      "7b042026561c4d09ac0ed0ce223887e6",
      "7220b74aa0fa41f5b1d5c3afe439404f",
      "00f7a48eae2645b79be440421dae883b",
      "1c2e955323244e2c9a647236d1197a7c",
      "cea54e453fdf4312b02f8ad0f2eefa23",
      "e061fac70999428e88ebd8b8aba04d00",
      "2d3d8cfc24444f3b8d5f4e0d171b54f8",
      "1e3e7fa896134d8c99c6deffddbccd23",
      "42fe954ac94444db8336169529e887ea",
      "b1cd80025844471fa735d43797bf74a3",
      "09ea81edb3964148b732831054b67cf7",
      "bbc8807380f541ad9673756b4c9d43a1",
      "645b5b18aa8b42479fd39403c85b12b0",
      "70b43b880af34c7aaa50f944b47b86db",
      "de5e9d23e9d34118ad298ae0e98b9d38",
      "98707cee27b44470b6892ed1485fd25d",
      "048a9d4b90294961b09d411affeffb3f",
      "cba52db70318478388521a9e88fd7919",
      "3b0cffe46f9d4105aa853591a5679f67",
      "91a0fe9fe65744ea825d2759c3a16a23"
     ]
    },
    "id": "ORa-rPvGmT9p",
    "outputId": "24fd81c0-753f-42e4-9981-f1ded870c07c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
      "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
      "To force `safe_serialization`, set it to `None` instead.\n",
      "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
      "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
      "Unsloth: Will remove a cached repo with size 2.2G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 4.07 out of 12.67 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:01<00:00, 27.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model/pytorch_model-00001-of-00002.bin...\n",
      "Unsloth: Saving model/pytorch_model-00002-of-00002.bin...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You are pushing to hub, but you passed your HF username = Chinnu1103.\n",
      "We shall truncate Chinnu1103/ROSLlama to ROSLlama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 3.99 out of 12.67 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:00<00:00, 30.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4521fc777b084c0d949b9333a09b4ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n",
      "Unsloth: Saving ROSLlama/pytorch_model-00001-of-00002.bin...\n",
      "Unsloth: Saving ROSLlama/pytorch_model-00002-of-00002.bin...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4844464e75c64d539282a9c5e18bb5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/581 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e15242e72bb459cad9651afdfd7c538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a477c801904d788ab851e8933c4a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cd80025844471fa735d43797bf74a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged model to https://huggingface.co/Chinnu1103/ROSLlama\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\n",
    "model.push_to_hub_merged(\"Chinnu1103/ROSLlama\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b4be1eb356284067a04e4357be316529",
      "40227403377841e9953a025e2e7d5117",
      "0ac6b3f256784554a2b3bdfa7ffdbb39",
      "d20bff7ceb1b4b7fa662d93b3e73d180",
      "2dfd49f6c04f453294a80aa043f3a604",
      "3ef85a703c4743b1a00f0105d977c92d",
      "06787cfb1b034a40b481442e6d768deb",
      "27747219775644ed8258d888f37ff40d",
      "9f160712e8d445ec8630e130c67ba107",
      "cec9fde0269f4db2a3b416f413bf2e08",
      "b14135a39e6b42b6b8333189fd244da9",
      "44b89cf060d94bdbb0d820b49e59376d",
      "8042775f77994044bdccee69d75faaed",
      "26d920b63e9c40b2b8019240c79838aa",
      "bea1590011ee40a2ae44c879ab96c457",
      "2376794c7fcf4ac9acb2cb5543b72ff8",
      "eb59b7cbb21a4ce2afcd37f0eeecd702",
      "c8d016d56d7f435386d312d0d4d858f8",
      "8fa07b747ae242d4998f41da01801765",
      "f3c13455b37c4a83a852c4d57ebd3b94",
      "6512fb9a7c854e18932df368987b834c",
      "0cba0ef673e8458bb7c45a80cbecab2d",
      "808ce182d587409ea5bf0a395bef7b23",
      "909a4cc9c23643e7b765ec42d37aac64",
      "2cc5339fda0648dc807c4443c3114bc3",
      "2c2a73aeb4e74051a85db2363ec18baf",
      "d029b54199184b6daa68ef58f7a84e6d",
      "69f9cfa24500498a833e131373854599",
      "39ce4c4d564e4e258d2cf854f23c79fd",
      "9bd2668b44b044fa8beabf65c1182746",
      "8b4632cbc89745e1b2d627b847188804",
      "e332ee6eb3694f18873c00e1f4309bd3",
      "cb9c116fe25948c7a2e142f4d03f6ddc",
      "9e6574310d6644b5a3d00976eb386b34",
      "0c3fd624e208478cbd1561b6a6790b3a",
      "44ab89efaf00487295786140c56f0ce7",
      "57613b8c2e6c42bc90da06471b615230",
      "f17f534631da4eadb1d52131f771c725",
      "731af7f94cc845a8b0151b483daa8f0f",
      "663e04b2d89f4965b50188ba504b4c80",
      "52a003fcc25442e1a876feda9d6a276f",
      "a504cb9149bb4ca58a16ad0ce3497f18",
      "60d29f73ea6d43be8805980eed866a87",
      "1399de05b0594221a0b3c454286bb473",
      "c328309e56724559bf44d80902e5e5ca",
      "498218ace5084083a61313c069029fe6",
      "7b163a9d66fa47229f4a53a0cba3f813",
      "f44e23c23aa7414398a90217dc1b62b1",
      "c896e080dd8b4caf8dda1e8dc2e40a9b",
      "6dc7e39f802b495c87a7e65853c7b6c3",
      "8b62ff31d1f0413b83120618ed9f4d9a",
      "7f78d99da8544a8698a24862904c0a8e",
      "50bbb5e909c0431fa13217c068c9c26c",
      "651e4d888af54cecbda5d6f1ed2d2361",
      "a942634538984b70ae240de767e2692f",
      "d9d057eeae744dc6a31b14d6efec1b25",
      "b25c541ce2904b71a944bfe9de08f79d",
      "75969b39388347fca02088a14df3fd26",
      "8ba60153b51f4d3b94dde94b969ef25f",
      "1cdfc61e90b146d4bec8d2c9117fa61a",
      "826c8ca05634406390cf8fae079c3ebd",
      "e0fc431f0dd3423d87c4e4766b7c715c",
      "6c77ed96a6b64e29a73e42dfec48d04f",
      "238d88eb9e5a4ce09d5a1e9bc014c0b0",
      "2d7f8814478b4d64b82b0cb2fd68d0da",
      "45ad425ac4c84ba9b4d6cd855a84106a"
     ]
    },
    "id": "fzcUOyksmgWH",
    "outputId": "59480bba-4fa2-4148-8d0c-73c561bb0b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 4.48 out of 12.67 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:01<00:00, 21.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model/pytorch_model-00001-of-00002.bin...\n",
      "Unsloth: Saving model/pytorch_model-00002-of-00002.bin...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting llama model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q8_0'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
      "Unsloth: [1] Converting model at model into q8_0 GGUF format.\n",
      "The output location will be /content/model/unsloth.Q8_0.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> Q8_0, shape = {3072, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 24\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting chat_template to {% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
      "' + message['content'] + '<|im_end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|im_start|>user\n",
      "' + message['value'] + '<|im_end|>\n",
      "'}}{% elif message['from'] == 'gpt' %}{{'<|im_start|>assistant\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/content/model/unsloth.Q8_0.gguf: n_tensors = 255, total_size = 3.4G\n",
      "Writing: 100%|| 3.41G/3.41G [01:15<00:00, 44.9Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /content/model/unsloth.Q8_0.gguf\n",
      "Unsloth: Conversion completed! Output location: /content/model/unsloth.Q8_0.gguf\n",
      "Unsloth: Saved Ollama Modelfile to model/Modelfile\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 4.49 out of 12.67 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:00<00:00, 29.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving Chinnu1103/ROSLlama-GGUF/pytorch_model-00001-of-00002.bin...\n",
      "Unsloth: Saving Chinnu1103/ROSLlama-GGUF/pytorch_model-00002-of-00002.bin...\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q2_k'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at Chinnu1103/ROSLlama-GGUF into f16 GGUF format.\n",
      "The output location will be /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: ROSLlama-GGUF\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 24\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting chat_template to {% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
      "' + message['content'] + '<|im_end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|im_start|>user\n",
      "' + message['value'] + '<|im_end|>\n",
      "'}}{% elif message['from'] == 'gpt' %}{{'<|im_start|>assistant\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf: n_tensors = 255, total_size = 6.4G\n",
      "Writing: 100%|| 6.43G/6.43G [01:56<00:00, 55.1Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "Unsloth: Conversion completed! Output location: /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q2_k. This might take 20 minutes...\n",
      "main: build = 4265 (59f4db10)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf' to '/content/Chinnu1103/ROSLlama-GGUF/unsloth.Q2_K.gguf' as Q2_K using 4 threads\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\" \", \" \", \" \", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% if 'role' in messages[0] %}{% for ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type  f16:  197 tensors\n",
      "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
      "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q2_K .. size =     6.00 MiB ->     0.98 MiB\n",
      "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q2_K .. size =    18.00 MiB ->     2.95 MiB\n",
      "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q2_K .. size =    48.00 MiB ->     7.88 MiB\n",
      "llama_model_quantize_internal: model size  =  6128.17 MB\n",
      "llama_model_quantize_internal: quant size  =  1293.28 MB\n",
      "\n",
      "main: quantize time = 234126.86 ms\n",
      "main:    total time = 234126.86 ms\n",
      "Unsloth: Conversion completed! Output location: /content/Chinnu1103/ROSLlama-GGUF/unsloth.Q2_K.gguf\n",
      "Unsloth: Saved Ollama Modelfile to Chinnu1103/ROSLlama-GGUF/Modelfile\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4be1eb356284067a04e4357be316529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q2_K.gguf:   0%|          | 0.00/1.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/Chinnu1103/ROSLlama-GGUF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Ollama Modelfile to https://huggingface.co/Chinnu1103/ROSLlama-GGUF\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 4.53 out of 12.67 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:00<00:00, 29.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving Chinnu1103/ROSLlama-GGUF/pytorch_model-00001-of-00002.bin...\n",
      "Unsloth: Saving Chinnu1103/ROSLlama-GGUF/pytorch_model-00002-of-00002.bin...\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q3_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at Chinnu1103/ROSLlama-GGUF into f16 GGUF format.\n",
      "The output location will be /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: ROSLlama-GGUF\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 24\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting chat_template to {% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
      "' + message['content'] + '<|im_end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|im_start|>user\n",
      "' + message['value'] + '<|im_end|>\n",
      "'}}{% elif message['from'] == 'gpt' %}{{'<|im_start|>assistant\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf: n_tensors = 255, total_size = 6.4G\n",
      "Writing: 100%|| 6.43G/6.43G [02:08<00:00, 49.9Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "Unsloth: Conversion completed! Output location: /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q3_k_m. This might take 20 minutes...\n",
      "main: build = 4265 (59f4db10)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf' to '/content/Chinnu1103/ROSLlama-GGUF/unsloth.Q3_K_M.gguf' as Q3_K_M using 4 threads\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\" \", \" \", \" \", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% if 'role' in messages[0] %}{% for ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type  f16:  197 tensors\n",
      "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
      "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q3_K .. size =     6.00 MiB ->     1.29 MiB\n",
      "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q3_K .. size =    18.00 MiB ->     3.87 MiB\n",
      "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q3_K .. size =    48.00 MiB ->    10.31 MiB\n",
      "llama_model_quantize_internal: model size  =  6128.17 MB\n",
      "llama_model_quantize_internal: quant size  =  1601.53 MB\n",
      "\n",
      "main: quantize time = 212837.03 ms\n",
      "main:    total time = 212837.03 ms\n",
      "Unsloth: Conversion completed! Output location: /content/Chinnu1103/ROSLlama-GGUF/unsloth.Q3_K_M.gguf\n",
      "Unsloth: Saved Ollama Modelfile to Chinnu1103/ROSLlama-GGUF/Modelfile\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b89cf060d94bdbb0d820b49e59376d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q3_K_M.gguf:   0%|          | 0.00/1.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/Chinnu1103/ROSLlama-GGUF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Ollama Modelfile to https://huggingface.co/Chinnu1103/ROSLlama-GGUF\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 4.53 out of 12.67 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:00<00:00, 30.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving Chinnu1103/ROSLlama-GGUF/pytorch_model-00001-of-00002.bin...\n",
      "Unsloth: Saving Chinnu1103/ROSLlama-GGUF/pytorch_model-00002-of-00002.bin...\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at Chinnu1103/ROSLlama-GGUF into f16 GGUF format.\n",
      "The output location will be /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: ROSLlama-GGUF\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 24\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting chat_template to {% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
      "' + message['content'] + '<|im_end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|im_start|>user\n",
      "' + message['value'] + '<|im_end|>\n",
      "'}}{% elif message['from'] == 'gpt' %}{{'<|im_start|>assistant\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf: n_tensors = 255, total_size = 6.4G\n",
      "Writing: 100%|| 6.43G/6.43G [02:09<00:00, 49.5Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "Unsloth: Conversion completed! Output location: /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
      "main: build = 4265 (59f4db10)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf' to '/content/Chinnu1103/ROSLlama-GGUF/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\" \", \" \", \" \", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% if 'role' in messages[0] %}{% for ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type  f16:  197 tensors\n",
      "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
      "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "llama_model_quantize_internal: model size  =  6128.17 MB\n",
      "llama_model_quantize_internal: quant size  =  1918.35 MB\n",
      "\n",
      "main: quantize time = 354583.36 ms\n",
      "main:    total time = 354583.36 ms\n",
      "Unsloth: Conversion completed! Output location: /content/Chinnu1103/ROSLlama-GGUF/unsloth.Q4_K_M.gguf\n",
      "Unsloth: Saved Ollama Modelfile to Chinnu1103/ROSLlama-GGUF/Modelfile\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808ce182d587409ea5bf0a395bef7b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q4_K_M.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/Chinnu1103/ROSLlama-GGUF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Ollama Modelfile to https://huggingface.co/Chinnu1103/ROSLlama-GGUF\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 4.53 out of 12.67 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:00<00:00, 29.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving Chinnu1103/ROSLlama-GGUF/pytorch_model-00001-of-00002.bin...\n",
      "Unsloth: Saving Chinnu1103/ROSLlama-GGUF/pytorch_model-00002-of-00002.bin...\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q5_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at Chinnu1103/ROSLlama-GGUF into f16 GGUF format.\n",
      "The output location will be /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: ROSLlama-GGUF\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 24\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting chat_template to {% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
      "' + message['content'] + '<|im_end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|im_start|>user\n",
      "' + message['value'] + '<|im_end|>\n",
      "'}}{% elif message['from'] == 'gpt' %}{{'<|im_start|>assistant\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf: n_tensors = 255, total_size = 6.4G\n",
      "Writing: 100%|| 6.43G/6.43G [02:04<00:00, 51.6Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "Unsloth: Conversion completed! Output location: /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q5_k_m. This might take 20 minutes...\n",
      "main: build = 4265 (59f4db10)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf' to '/content/Chinnu1103/ROSLlama-GGUF/unsloth.Q5_K_M.gguf' as Q5_K_M using 4 threads\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\" \", \" \", \" \", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% if 'role' in messages[0] %}{% for ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type  f16:  197 tensors\n",
      "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
      "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "llama_model_quantize_internal: model size  =  6128.17 MB\n",
      "llama_model_quantize_internal: quant size  =  2207.10 MB\n",
      "\n",
      "main: quantize time = 295602.98 ms\n",
      "main:    total time = 295602.98 ms\n",
      "Unsloth: Conversion completed! Output location: /content/Chinnu1103/ROSLlama-GGUF/unsloth.Q5_K_M.gguf\n",
      "Unsloth: Saved Ollama Modelfile to Chinnu1103/ROSLlama-GGUF/Modelfile\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6574310d6644b5a3d00976eb386b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q5_K_M.gguf:   0%|          | 0.00/2.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/Chinnu1103/ROSLlama-GGUF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Ollama Modelfile to https://huggingface.co/Chinnu1103/ROSLlama-GGUF\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 5.47 out of 12.67 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:00<00:00, 29.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving Chinnu1103/ROSLlama-GGUF/pytorch_model-00001-of-00002.bin...\n",
      "Unsloth: Saving Chinnu1103/ROSLlama-GGUF/pytorch_model-00002-of-00002.bin...\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q6_k'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at Chinnu1103/ROSLlama-GGUF into f16 GGUF format.\n",
      "The output location will be /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: ROSLlama-GGUF\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 24\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting chat_template to {% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
      "' + message['content'] + '<|im_end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|im_start|>user\n",
      "' + message['value'] + '<|im_end|>\n",
      "'}}{% elif message['from'] == 'gpt' %}{{'<|im_start|>assistant\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf: n_tensors = 255, total_size = 6.4G\n",
      "Writing: 100%|| 6.43G/6.43G [01:34<00:00, 68.0Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "Unsloth: Conversion completed! Output location: /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q6_k. This might take 20 minutes...\n",
      "main: build = 4265 (59f4db10)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf' to '/content/Chinnu1103/ROSLlama-GGUF/unsloth.Q6_K.gguf' as Q6_K using 4 threads\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/Chinnu1103/ROSLlama-GGUF/unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\" \", \" \", \" \", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% if 'role' in messages[0] %}{% for ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type  f16:  197 tensors\n",
      "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
      "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "llama_model_quantize_internal: model size  =  6128.17 MB\n",
      "llama_model_quantize_internal: quant size  =  2513.90 MB\n",
      "\n",
      "main: quantize time = 171428.54 ms\n",
      "main:    total time = 171428.54 ms\n",
      "Unsloth: Conversion completed! Output location: /content/Chinnu1103/ROSLlama-GGUF/unsloth.Q6_K.gguf\n",
      "Unsloth: Saved Ollama Modelfile to Chinnu1103/ROSLlama-GGUF/Modelfile\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c328309e56724559bf44d80902e5e5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q6_K.gguf:   0%|          | 0.00/2.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/Chinnu1103/ROSLlama-GGUF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Ollama Modelfile to https://huggingface.co/Chinnu1103/ROSLlama-GGUF\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 5.46 out of 12.67 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:00<00:00, 30.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving Chinnu1103/ROSLlama-GGUF/pytorch_model-00001-of-00002.bin...\n",
      "Unsloth: Saving Chinnu1103/ROSLlama-GGUF/pytorch_model-00002-of-00002.bin...\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q8_0'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at Chinnu1103/ROSLlama-GGUF into q8_0 GGUF format.\n",
      "The output location will be /content/Chinnu1103/ROSLlama-GGUF/unsloth.Q8_0.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: ROSLlama-GGUF\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> Q8_0, shape = {3072, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 24\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting chat_template to {% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
      "' + message['content'] + '<|im_end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|im_start|>user\n",
      "' + message['value'] + '<|im_end|>\n",
      "'}}{% elif message['from'] == 'gpt' %}{{'<|im_start|>assistant\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['value'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/content/Chinnu1103/ROSLlama-GGUF/unsloth.Q8_0.gguf: n_tensors = 255, total_size = 3.4G\n",
      "Writing: 100%|| 3.41G/3.41G [01:08<00:00, 49.8Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /content/Chinnu1103/ROSLlama-GGUF/unsloth.Q8_0.gguf\n",
      "Unsloth: Conversion completed! Output location: /content/Chinnu1103/ROSLlama-GGUF/unsloth.Q8_0.gguf\n",
      "Unsloth: Saved Ollama Modelfile to Chinnu1103/ROSLlama-GGUF/Modelfile\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d057eeae744dc6a31b14d6efec1b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q8_0.gguf:   0%|          | 0.00/3.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/Chinnu1103/ROSLlama-GGUF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Ollama Modelfile to https://huggingface.co/Chinnu1103/ROSLlama-GGUF\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"model\", tokenizer, \"q8_0\")\n",
    "quant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\n",
    "for quant in quant_methods:\n",
    "    model.push_to_hub_gguf(\"Chinnu1103/ROSLlama-GGUF\", tokenizer, quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NhvrjUp4Zuyf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('screenshots/finetuning_report.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00c0e279dbfe4f7c8cd83b81a59ddd55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3b36a7460fe42809f54c6fb08d0a6d7",
      "max": 1459746080,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_64aa5c3796454cb7a1877a8855bf87e5",
      "value": 1459746080
     }
    },
    "00f7a48eae2645b79be440421dae883b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "048a9d4b90294961b09d411affeffb3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d850bc165f430f91da2ffc32f6db74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04f443a12db34e3abb07bac61e6e0391": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0561f19d2fbe4aef801c8fb536f41b9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54380f27cf3444e99f5d2c741ce41b92",
      "placeholder": "",
      "style": "IPY_MODEL_2b1357a7590649cf94c90c12eb6e60be",
      "value": "2.24G/2.24G[00:53&lt;00:00,42.9MB/s]"
     }
    },
    "06787cfb1b034a40b481442e6d768deb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06e8bc0345bf42ab90c7df239359a5c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ea5cd71a6684bb897a41e748e39103b",
      "max": 581,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cdf5969d7f584dabb34bd41937319c1e",
      "value": 581
     }
    },
    "089f004dd0044d6d816705ad0bdb0a0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc0d751538294908ab6aac0e2d744f2f",
      "placeholder": "",
      "style": "IPY_MODEL_3233b05efc58446a9c05f3723ad06e19",
      "value": "951/951[00:00&lt;00:00,4608.79examples/s]"
     }
    },
    "0957db7f37784779b5d186efc1f6eaad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89a2149e2598483094e43bf20acc706a",
       "IPY_MODEL_5a90bb8ab2174aefbf9de8c303f60d6c",
       "IPY_MODEL_c54191c987d8434a87c31e06287727f4"
      ],
      "layout": "IPY_MODEL_bfccf8c5d0d94c78bb44fca4130123b3"
     }
    },
    "09ea81edb3964148b732831054b67cf7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de5e9d23e9d34118ad298ae0e98b9d38",
      "placeholder": "",
      "style": "IPY_MODEL_98707cee27b44470b6892ed1485fd25d",
      "value": "pytorch_model-00001-of-00002.bin:100%"
     }
    },
    "0ac6b3f256784554a2b3bdfa7ffdbb39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27747219775644ed8258d888f37ff40d",
      "max": 1363932320,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f160712e8d445ec8630e130c67ba107",
      "value": 1363932320
     }
    },
    "0b588b219e9c464f9bd7a420e169d900": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c3fd624e208478cbd1561b6a6790b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_731af7f94cc845a8b0151b483daa8f0f",
      "placeholder": "",
      "style": "IPY_MODEL_663e04b2d89f4965b50188ba504b4c80",
      "value": "unsloth.Q5_K_M.gguf:100%"
     }
    },
    "0cba0ef673e8458bb7c45a80cbecab2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f182d0722a8492f9052c0b96dee1a67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c441f839175849dc9e5053817b04ec38",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4641a1357c9e413da26a8ab39599708e",
      "value": 1
     }
    },
    "0f4a16d145d94095a8ae81c8d09e9c04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11258a344fa041de8b8c522956ff8ed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b9348e5c677446dbdc5fc2921049b85",
      "max": 459,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e138b17d46e94b26846a37db4903e01d",
      "value": 459
     }
    },
    "11a683b037ff4310b7cc1bf7d0e9e3e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc044cdc0e6b4e61b8437b3d08b4c09e",
      "placeholder": "",
      "style": "IPY_MODEL_8adf1a9e656a4a69876eff77d0f3f851",
      "value": "model.safetensors:100%"
     }
    },
    "1399de05b0594221a0b3c454286bb473": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "187112d231374cb49bc7a399f7145985": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04d850bc165f430f91da2ffc32f6db74",
      "placeholder": "",
      "style": "IPY_MODEL_f9f8068a2fbf42b089d35443494e87ae",
      "value": "special_tokens_map.json:100%"
     }
    },
    "18d82fbd47d14d94a9a3f7676681e76c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1becbf72632d483b83241b72d36d2b36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c2e955323244e2c9a647236d1197a7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cdfc61e90b146d4bec8d2c9117fa61a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e3e7fa896134d8c99c6deffddbccd23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2090dbc4252947ef8d0ceacf98ff2062": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20b668f5021a4522b8966afab2e52702": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20b91636b855448ab2d5803a7e447b37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a7aef730a0e4487bc15a41e9fa2e838",
      "placeholder": "",
      "style": "IPY_MODEL_be7bc22e35674924b2e416042ffd03f5",
      "value": "Generatingtrainsplit:"
     }
    },
    "21dc5bf8443447e9a6e7952fada7a347": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2376794c7fcf4ac9acb2cb5543b72ff8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "238d88eb9e5a4ce09d5a1e9bc014c0b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2594998316c248c59701868949591fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25a50ae9d35a4e5d92a096b000a96ea3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "26d920b63e9c40b2b8019240c79838aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fa07b747ae242d4998f41da01801765",
      "max": 1687155872,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3c13455b37c4a83a852c4d57ebd3b94",
      "value": 1687155872
     }
    },
    "27747219775644ed8258d888f37ff40d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "280c1fdbb9e544df8ab776329b18fca9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78de5339cdd244b5b803c3fcc0cb19fd",
       "IPY_MODEL_0f182d0722a8492f9052c0b96dee1a67",
       "IPY_MODEL_63bdcfb65cde463a9e57c0d5dbb764d2"
      ],
      "layout": "IPY_MODEL_871b04f4b63b479d9cbc337668493146"
     }
    },
    "2b1357a7590649cf94c90c12eb6e60be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c2a73aeb4e74051a85db2363ec18baf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e332ee6eb3694f18873c00e1f4309bd3",
      "placeholder": "",
      "style": "IPY_MODEL_cb9c116fe25948c7a2e142f4d03f6ddc",
      "value": "2.02G/2.02G[00:51&lt;00:00,56.5MB/s]"
     }
    },
    "2cc5339fda0648dc807c4443c3114bc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bd2668b44b044fa8beabf65c1182746",
      "max": 2019374240,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b4632cbc89745e1b2d627b847188804",
      "value": 2019374240
     }
    },
    "2d3d8cfc24444f3b8d5f4e0d171b54f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2d7f8814478b4d64b82b0cb2fd68d0da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dfd49f6c04f453294a80aa043f3a604": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e0776c5c8fa43a3be7dd7e91ec1c185": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3233b05efc58446a9c05f3723ad06e19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34a85108be5644dcb2ea0c861c1b69c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "35752176c09147359968d9e065966775": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8cf4811b5b2048eeaed6daf7f1a47764",
       "IPY_MODEL_6460b8ff42d34bf992eb66da8049bf35",
       "IPY_MODEL_089f004dd0044d6d816705ad0bdb0a0f"
      ],
      "layout": "IPY_MODEL_7889adbe472e46268e9d56d6e4692ff6"
     }
    },
    "35887fa1ec4e4e0b92f918c803ce48ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "359831ded9074dbc82084504ab66d5a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39ce4c4d564e4e258d2cf854f23c79fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b0cffe46f9d4105aa853591a5679f67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c3c328d9ee6426b9109a3e324d488c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d10a773e6bb4a0da8f14d7e4016b32a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_11a683b037ff4310b7cc1bf7d0e9e3e2",
       "IPY_MODEL_8825a0ca10654ddeaf7c58c1544953a0",
       "IPY_MODEL_0561f19d2fbe4aef801c8fb536f41b9f"
      ],
      "layout": "IPY_MODEL_caa394df371847cebf497fa030bb27b2"
     }
    },
    "3e15242e72bb459cad9651afdfd7c538": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_41cb3d35829b47318fa293480dc22b2e",
       "IPY_MODEL_00c0e279dbfe4f7c8cd83b81a59ddd55",
       "IPY_MODEL_9c1fee69dd10400095cf06324c86cb6c"
      ],
      "layout": "IPY_MODEL_3c3c328d9ee6426b9109a3e324d488c3"
     }
    },
    "3ef85a703c4743b1a00f0105d977c92d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fd3fe6709914b56a0bd8e0cdfd5e329": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b17e144fd98b40c298725c06d7709642",
      "placeholder": "",
      "style": "IPY_MODEL_c2ba5a3233e54b4daf095727468486e5",
      "value": "tokenizer.json:100%"
     }
    },
    "40227403377841e9953a025e2e7d5117": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ef85a703c4743b1a00f0105d977c92d",
      "placeholder": "",
      "style": "IPY_MODEL_06787cfb1b034a40b481442e6d768deb",
      "value": "unsloth.Q2_K.gguf:100%"
     }
    },
    "41cb3d35829b47318fa293480dc22b2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_359831ded9074dbc82084504ab66d5a8",
      "placeholder": "",
      "style": "IPY_MODEL_20b668f5021a4522b8966afab2e52702",
      "value": "pytorch_model-00002-of-00002.bin:100%"
     }
    },
    "42fe954ac94444db8336169529e887ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "431c91f76bdb4d30b66617d106d61cf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "44ab89efaf00487295786140c56f0ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52a003fcc25442e1a876feda9d6a276f",
      "max": 2322150560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a504cb9149bb4ca58a16ad0ce3497f18",
      "value": 2322150560
     }
    },
    "44b89cf060d94bdbb0d820b49e59376d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8042775f77994044bdccee69d75faaed",
       "IPY_MODEL_26d920b63e9c40b2b8019240c79838aa",
       "IPY_MODEL_bea1590011ee40a2ae44c879ab96c457"
      ],
      "layout": "IPY_MODEL_2376794c7fcf4ac9acb2cb5543b72ff8"
     }
    },
    "4521fc777b084c0d949b9333a09b4ef0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_feed91a06a1143648e570b35392b1858",
       "IPY_MODEL_fe2b3f93035b4fb58c355721ff2faaa3",
       "IPY_MODEL_790377199f3544c2856225c388123bbf"
      ],
      "layout": "IPY_MODEL_fba5a1acc6564da7b539379a01bb7eb2"
     }
    },
    "45ad425ac4c84ba9b4d6cd855a84106a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45c2c5f77397453a9f5c5cd99af303e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45e034b70899436d9bdaca03d97ee056": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4641a1357c9e413da26a8ab39599708e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4729a9af410b43ceacc62fab0b4ac300": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4844464e75c64d539282a9c5e18bb5e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aec675d6c87b429e8944d652519f7b55",
       "IPY_MODEL_06e8bc0345bf42ab90c7df239359a5c2",
       "IPY_MODEL_ded49622d0644be9ab266b777c109975"
      ],
      "layout": "IPY_MODEL_525d4c171a304224a8826afbdbe5e765"
     }
    },
    "498218ace5084083a61313c069029fe6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6dc7e39f802b495c87a7e65853c7b6c3",
      "placeholder": "",
      "style": "IPY_MODEL_8b62ff31d1f0413b83120618ed9f4d9a",
      "value": "unsloth.Q6_K.gguf:100%"
     }
    },
    "4b76d52495204447aa80c9faf28dfb5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b9e0ea2499a4d55b27675d0b5834136": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4cdbe71cbc744df9b041c4b23002257e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "50bbb5e909c0431fa13217c068c9c26c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "525d4c171a304224a8826afbdbe5e765": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52a003fcc25442e1a876feda9d6a276f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54380f27cf3444e99f5d2c741ce41b92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57613b8c2e6c42bc90da06471b615230": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60d29f73ea6d43be8805980eed866a87",
      "placeholder": "",
      "style": "IPY_MODEL_1399de05b0594221a0b3c454286bb473",
      "value": "2.32G/2.32G[00:55&lt;00:00,59.2MB/s]"
     }
    },
    "57c4b69a3d074583baeeaa0dcbd9e4a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59b1e8ad643247c5850a16a65ba93a02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a90bb8ab2174aefbf9de8c303f60d6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59b1e8ad643247c5850a16a65ba93a02",
      "max": 50570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d67b0928681947f98a27b68898cdcd56",
      "value": 50570
     }
    },
    "603730ade2eb4dbf8903e3ab10ad9cd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e82bb6497bdd41bd8b54628d1ee87c10",
       "IPY_MODEL_e8cd6d9e2ce64c69baa7cee8eda03698",
       "IPY_MODEL_9fdf5ef1792a490f841d24ec849f4cfe"
      ],
      "layout": "IPY_MODEL_04f443a12db34e3abb07bac61e6e0391"
     }
    },
    "60d29f73ea6d43be8805980eed866a87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61c9d92a11744ee0aa1bfa201d5897f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63bdcfb65cde463a9e57c0d5dbb764d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4c2aac0f0d04bda81482db0c0869d06",
      "placeholder": "",
      "style": "IPY_MODEL_34a85108be5644dcb2ea0c861c1b69c5",
      "value": "500/0[00:02&lt;00:00,327.76examples/s]"
     }
    },
    "645b5b18aa8b42479fd39403c85b12b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b0cffe46f9d4105aa853591a5679f67",
      "placeholder": "",
      "style": "IPY_MODEL_91a0fe9fe65744ea825d2759c3a16a23",
      "value": "4.97G/4.97G[01:56&lt;00:00,44.1MB/s]"
     }
    },
    "6460b8ff42d34bf992eb66da8049bf35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68446edbfdcb4b2cb3ecfbf57bddd270",
      "max": 951,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be50c29e17a34da2b3b5fbbc63a84cf1",
      "value": 951
     }
    },
    "64aa5c3796454cb7a1877a8855bf87e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6512fb9a7c854e18932df368987b834c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "651e4d888af54cecbda5d6f1ed2d2361": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66370192cdcb4d288fc08dfc6c1bb164": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "663e04b2d89f4965b50188ba504b4c80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68446edbfdcb4b2cb3ecfbf57bddd270": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "694d8c0c8fa24660949a2d0029127c29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "697c34160273411490d799c21fc08317": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "69f9cfa24500498a833e131373854599": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a7aef730a0e4487bc15a41e9fa2e838": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c77ed96a6b64e29a73e42dfec48d04f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6dc7e39f802b495c87a7e65853c7b6c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7091c107535344468537ab53ba4aee78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc0278626d8b4b748572a7d43ac0aa0f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7524f4ee46de444c8a2f363a59bfa375",
      "value": 1
     }
    },
    "70b43b880af34c7aaa50f944b47b86db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7220b74aa0fa41f5b1d5c3afe439404f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e3e7fa896134d8c99c6deffddbccd23",
      "placeholder": "",
      "style": "IPY_MODEL_42fe954ac94444db8336169529e887ea",
      "value": "2/2[01:57&lt;00:00,117.23s/it]"
     }
    },
    "731af7f94cc845a8b0151b483daa8f0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7524f4ee46de444c8a2f363a59bfa375": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7587b5e7516448e6a9fa6d97be0159ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75969b39388347fca02088a14df3fd26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c77ed96a6b64e29a73e42dfec48d04f",
      "max": 3421895840,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_238d88eb9e5a4ce09d5a1e9bc014c0b0",
      "value": 3421895840
     }
    },
    "7889adbe472e46268e9d56d6e4692ff6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78de5339cdd244b5b803c3fcc0cb19fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f4a16d145d94095a8ae81c8d09e9c04",
      "placeholder": "",
      "style": "IPY_MODEL_2594998316c248c59701868949591fca",
      "value": "Generatingtrainsplit:"
     }
    },
    "790377199f3544c2856225c388123bbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cda1c002fc084bfc9b0b9d42f53cd251",
      "placeholder": "",
      "style": "IPY_MODEL_a79742df9853489c99d8230e37957718",
      "value": "17.2M/17.2M[00:01&lt;00:00,33.7MB/s]"
     }
    },
    "7b042026561c4d09ac0ed0ce223887e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e061fac70999428e88ebd8b8aba04d00",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2d3d8cfc24444f3b8d5f4e0d171b54f8",
      "value": 2
     }
    },
    "7b163a9d66fa47229f4a53a0cba3f813": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f78d99da8544a8698a24862904c0a8e",
      "max": 2643850400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_50bbb5e909c0431fa13217c068c9c26c",
      "value": 2643850400
     }
    },
    "7f78d99da8544a8698a24862904c0a8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8042775f77994044bdccee69d75faaed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb59b7cbb21a4ce2afcd37f0eeecd702",
      "placeholder": "",
      "style": "IPY_MODEL_c8d016d56d7f435386d312d0d4d858f8",
      "value": "unsloth.Q3_K_M.gguf:100%"
     }
    },
    "80658efd6e324970b3db5477690bf5f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "808ce182d587409ea5bf0a395bef7b23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_909a4cc9c23643e7b765ec42d37aac64",
       "IPY_MODEL_2cc5339fda0648dc807c4443c3114bc3",
       "IPY_MODEL_2c2a73aeb4e74051a85db2363ec18baf"
      ],
      "layout": "IPY_MODEL_d029b54199184b6daa68ef58f7a84e6d"
     }
    },
    "826c8ca05634406390cf8fae079c3ebd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8514de2b5fb64f8b89408d594516c4f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_20b91636b855448ab2d5803a7e447b37",
       "IPY_MODEL_7091c107535344468537ab53ba4aee78",
       "IPY_MODEL_e0329fc5e2104d8295aa2d11d0ef5c41"
      ],
      "layout": "IPY_MODEL_2090dbc4252947ef8d0ceacf98ff2062"
     }
    },
    "86de80a1bb9343319b9a199f38c2ee8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "871b04f4b63b479d9cbc337668493146": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8825a0ca10654ddeaf7c58c1544953a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b514bece60b14840a668295a5ea0fc80",
      "max": 2242762785,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_697c34160273411490d799c21fc08317",
      "value": 2242762785
     }
    },
    "8881cfd89b7f49ba89e13b9c3da2a309": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_187112d231374cb49bc7a399f7145985",
       "IPY_MODEL_11258a344fa041de8b8c522956ff8ed6",
       "IPY_MODEL_cbe34cb9b7c7464fa2649ed8e2521164"
      ],
      "layout": "IPY_MODEL_f45356bc44464bc78f29bb1dd387dd3c"
     }
    },
    "89a2149e2598483094e43bf20acc706a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1becbf72632d483b83241b72d36d2b36",
      "placeholder": "",
      "style": "IPY_MODEL_9956572a4a004623865f3965588dd1c4",
      "value": "tokenizer_config.json:100%"
     }
    },
    "8adf1a9e656a4a69876eff77d0f3f851": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b4632cbc89745e1b2d627b847188804": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8b62ff31d1f0413b83120618ed9f4d9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ba60153b51f4d3b94dde94b969ef25f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d7f8814478b4d64b82b0cb2fd68d0da",
      "placeholder": "",
      "style": "IPY_MODEL_45ad425ac4c84ba9b4d6cd855a84106a",
      "value": "3.42G/3.42G[01:15&lt;00:00,54.9MB/s]"
     }
    },
    "8cf4811b5b2048eeaed6daf7f1a47764": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7587b5e7516448e6a9fa6d97be0159ad",
      "placeholder": "",
      "style": "IPY_MODEL_61c9d92a11744ee0aa1bfa201d5897f6",
      "value": "Map:100%"
     }
    },
    "8fa07b747ae242d4998f41da01801765": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "909a4cc9c23643e7b765ec42d37aac64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69f9cfa24500498a833e131373854599",
      "placeholder": "",
      "style": "IPY_MODEL_39ce4c4d564e4e258d2cf854f23c79fd",
      "value": "unsloth.Q4_K_M.gguf:100%"
     }
    },
    "91a0fe9fe65744ea825d2759c3a16a23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "95df967616fa485caee4966bea81d112": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "975b12029a3d45ce9aff069dc42a03a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98707cee27b44470b6892ed1485fd25d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9956572a4a004623865f3965588dd1c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9b9348e5c677446dbdc5fc2921049b85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bd2668b44b044fa8beabf65c1182746": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c1fee69dd10400095cf06324c86cb6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18d82fbd47d14d94a9a3f7676681e76c",
      "placeholder": "",
      "style": "IPY_MODEL_95df967616fa485caee4966bea81d112",
      "value": "1.46G/1.46G[00:34&lt;00:00,41.1MB/s]"
     }
    },
    "9e6574310d6644b5a3d00976eb386b34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0c3fd624e208478cbd1561b6a6790b3a",
       "IPY_MODEL_44ab89efaf00487295786140c56f0ce7",
       "IPY_MODEL_57613b8c2e6c42bc90da06471b615230"
      ],
      "layout": "IPY_MODEL_f17f534631da4eadb1d52131f771c725"
     }
    },
    "9ea5cd71a6684bb897a41e748e39103b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f160712e8d445ec8630e130c67ba107": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9fdf5ef1792a490f841d24ec849f4cfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd8992c81c294beea484f514269ae00e",
      "placeholder": "",
      "style": "IPY_MODEL_25a50ae9d35a4e5d92a096b000a96ea3",
      "value": "121/121[00:00&lt;00:00,4.10kB/s]"
     }
    },
    "a3b36a7460fe42809f54c6fb08d0a6d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a504cb9149bb4ca58a16ad0ce3497f18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a79742df9853489c99d8230e37957718": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a942634538984b70ae240de767e2692f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab30c2a8c3834a8eb175b3c1332f42d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c2e955323244e2c9a647236d1197a7c",
      "placeholder": "",
      "style": "IPY_MODEL_cea54e453fdf4312b02f8ad0f2eefa23",
      "value": "Upload2LFSfiles:100%"
     }
    },
    "aec675d6c87b429e8944d652519f7b55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66370192cdcb4d288fc08dfc6c1bb164",
      "placeholder": "",
      "style": "IPY_MODEL_4b76d52495204447aa80c9faf28dfb5b",
      "value": "README.md:100%"
     }
    },
    "b11ad008e6a64a4c8bff36fc0eb8c456": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b14135a39e6b42b6b8333189fd244da9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b17e144fd98b40c298725c06d7709642": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1cd80025844471fa735d43797bf74a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_09ea81edb3964148b732831054b67cf7",
       "IPY_MODEL_bbc8807380f541ad9673756b4c9d43a1",
       "IPY_MODEL_645b5b18aa8b42479fd39403c85b12b0"
      ],
      "layout": "IPY_MODEL_70b43b880af34c7aaa50f944b47b86db"
     }
    },
    "b25c541ce2904b71a944bfe9de08f79d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_826c8ca05634406390cf8fae079c3ebd",
      "placeholder": "",
      "style": "IPY_MODEL_e0fc431f0dd3423d87c4e4766b7c715c",
      "value": "unsloth.Q8_0.gguf:100%"
     }
    },
    "b4be1eb356284067a04e4357be316529": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_40227403377841e9953a025e2e7d5117",
       "IPY_MODEL_0ac6b3f256784554a2b3bdfa7ffdbb39",
       "IPY_MODEL_d20bff7ceb1b4b7fa662d93b3e73d180"
      ],
      "layout": "IPY_MODEL_2dfd49f6c04f453294a80aa043f3a604"
     }
    },
    "b514bece60b14840a668295a5ea0fc80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7a477c801904d788ab851e8933c4a7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ab30c2a8c3834a8eb175b3c1332f42d5",
       "IPY_MODEL_7b042026561c4d09ac0ed0ce223887e6",
       "IPY_MODEL_7220b74aa0fa41f5b1d5c3afe439404f"
      ],
      "layout": "IPY_MODEL_00f7a48eae2645b79be440421dae883b"
     }
    },
    "b7dd0760ff5441e28d3c8268df06d602": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbc8807380f541ad9673756b4c9d43a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_048a9d4b90294961b09d411affeffb3f",
      "max": 4965844039,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cba52db70318478388521a9e88fd7919",
      "value": 4965844039
     }
    },
    "bc044cdc0e6b4e61b8437b3d08b4c09e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be3207f979254634b612b26f53b095d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be50c29e17a34da2b3b5fbbc63a84cf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "be7bc22e35674924b2e416042ffd03f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bea1590011ee40a2ae44c879ab96c457": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6512fb9a7c854e18932df368987b834c",
      "placeholder": "",
      "style": "IPY_MODEL_0cba0ef673e8458bb7c45a80cbecab2d",
      "value": "1.69G/1.69G[00:49&lt;00:00,48.3MB/s]"
     }
    },
    "bfccf8c5d0d94c78bb44fca4130123b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2ba5a3233e54b4daf095727468486e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c328309e56724559bf44d80902e5e5ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_498218ace5084083a61313c069029fe6",
       "IPY_MODEL_7b163a9d66fa47229f4a53a0cba3f813",
       "IPY_MODEL_f44e23c23aa7414398a90217dc1b62b1"
      ],
      "layout": "IPY_MODEL_c896e080dd8b4caf8dda1e8dc2e40a9b"
     }
    },
    "c441f839175849dc9e5053817b04ec38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "c54191c987d8434a87c31e06287727f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45e034b70899436d9bdaca03d97ee056",
      "placeholder": "",
      "style": "IPY_MODEL_80658efd6e324970b3db5477690bf5f0",
      "value": "50.6k/50.6k[00:00&lt;00:00,1.08MB/s]"
     }
    },
    "c5f7635b2bc44957ab47b6e703d4ad89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3fd3fe6709914b56a0bd8e0cdfd5e329",
       "IPY_MODEL_dace7d2aff8144778f7ec38669dfbe61",
       "IPY_MODEL_ddee132a06514e8982b53f6ad18bfff6"
      ],
      "layout": "IPY_MODEL_f4346b679d4e443f976552cdb875e1dd"
     }
    },
    "c896e080dd8b4caf8dda1e8dc2e40a9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8d016d56d7f435386d312d0d4d858f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "caa394df371847cebf497fa030bb27b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb9c116fe25948c7a2e142f4d03f6ddc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cba52db70318478388521a9e88fd7919": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cba9738b4096450988f4d3cf22b4a0de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbe34cb9b7c7464fa2649ed8e2521164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7dd0760ff5441e28d3c8268df06d602",
      "placeholder": "",
      "style": "IPY_MODEL_4b9e0ea2499a4d55b27675d0b5834136",
      "value": "459/459[00:00&lt;00:00,20.7kB/s]"
     }
    },
    "cc0278626d8b4b748572a7d43ac0aa0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "cc0d751538294908ab6aac0e2d744f2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd8992c81c294beea484f514269ae00e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cda1c002fc084bfc9b0b9d42f53cd251": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdf5969d7f584dabb34bd41937319c1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cea54e453fdf4312b02f8ad0f2eefa23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cec9fde0269f4db2a3b416f413bf2e08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d029b54199184b6daa68ef58f7a84e6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d20bff7ceb1b4b7fa662d93b3e73d180": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cec9fde0269f4db2a3b416f413bf2e08",
      "placeholder": "",
      "style": "IPY_MODEL_b14135a39e6b42b6b8333189fd244da9",
      "value": "1.36G/1.36G[00:36&lt;00:00,26.6MB/s]"
     }
    },
    "d4c2aac0f0d04bda81482db0c0869d06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d67b0928681947f98a27b68898cdcd56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d9d057eeae744dc6a31b14d6efec1b25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b25c541ce2904b71a944bfe9de08f79d",
       "IPY_MODEL_75969b39388347fca02088a14df3fd26",
       "IPY_MODEL_8ba60153b51f4d3b94dde94b969ef25f"
      ],
      "layout": "IPY_MODEL_1cdfc61e90b146d4bec8d2c9117fa61a"
     }
    },
    "dace7d2aff8144778f7ec38669dfbe61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57c4b69a3d074583baeeaa0dcbd9e4a7",
      "max": 9085657,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_431c91f76bdb4d30b66617d106d61cf2",
      "value": 9085657
     }
    },
    "ddee132a06514e8982b53f6ad18bfff6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e0776c5c8fa43a3be7dd7e91ec1c185",
      "placeholder": "",
      "style": "IPY_MODEL_4cdbe71cbc744df9b041c4b23002257e",
      "value": "9.09M/9.09M[00:00&lt;00:00,30.8MB/s]"
     }
    },
    "de5e9d23e9d34118ad298ae0e98b9d38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ded49622d0644be9ab266b777c109975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b588b219e9c464f9bd7a420e169d900",
      "placeholder": "",
      "style": "IPY_MODEL_86de80a1bb9343319b9a199f38c2ee8c",
      "value": "581/581[00:00&lt;00:00,30.7kB/s]"
     }
    },
    "dfb2ba18082446f09b7bdc237dc6aa25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0329fc5e2104d8295aa2d11d0ef5c41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be3207f979254634b612b26f53b095d5",
      "placeholder": "",
      "style": "IPY_MODEL_975b12029a3d45ce9aff069dc42a03a0",
      "value": "951/0[00:00&lt;00:00,2768.75examples/s]"
     }
    },
    "e061fac70999428e88ebd8b8aba04d00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0fc431f0dd3423d87c4e4766b7c715c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e138b17d46e94b26846a37db4903e01d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e332ee6eb3694f18873c00e1f4309bd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e82bb6497bdd41bd8b54628d1ee87c10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_694d8c0c8fa24660949a2d0029127c29",
      "placeholder": "",
      "style": "IPY_MODEL_b11ad008e6a64a4c8bff36fc0eb8c456",
      "value": "generation_config.json:100%"
     }
    },
    "e8cd6d9e2ce64c69baa7cee8eda03698": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45c2c5f77397453a9f5c5cd99af303e0",
      "max": 121,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_21dc5bf8443447e9a6e7952fada7a347",
      "value": 121
     }
    },
    "eb59b7cbb21a4ce2afcd37f0eeecd702": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f17f534631da4eadb1d52131f771c725": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3c13455b37c4a83a852c4d57ebd3b94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f4346b679d4e443f976552cdb875e1dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f44e23c23aa7414398a90217dc1b62b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_651e4d888af54cecbda5d6f1ed2d2361",
      "placeholder": "",
      "style": "IPY_MODEL_a942634538984b70ae240de767e2692f",
      "value": "2.64G/2.64G[01:05&lt;00:00,52.2MB/s]"
     }
    },
    "f45356bc44464bc78f29bb1dd387dd3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9f8068a2fbf42b089d35443494e87ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fba5a1acc6564da7b539379a01bb7eb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe2b3f93035b4fb58c355721ff2faaa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35887fa1ec4e4e0b92f918c803ce48ac",
      "max": 17209915,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4729a9af410b43ceacc62fab0b4ac300",
      "value": 17209915
     }
    },
    "feed91a06a1143648e570b35392b1858": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cba9738b4096450988f4d3cf22b4a0de",
      "placeholder": "",
      "style": "IPY_MODEL_dfb2ba18082446f09b7bdc237dc6aa25",
      "value": "tokenizer.json:100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
