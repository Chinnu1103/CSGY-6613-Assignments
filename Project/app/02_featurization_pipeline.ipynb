{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization Pipelines Milestone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries and initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from qdrant_client.http.models import PointStruct\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from qdrant_client import QdrantClient\n",
    "from pymongo import MongoClient\n",
    "from clearml import Task\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoTokenizer.parallelism = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize mongodb\n",
    "\n",
    "client = MongoClient(\"mongodb://mongo:27017/\")\n",
    "db = client[\"ros2_docs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATGPT_API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "CHATGPT_API_KEY = \"sk-K5Fr76TbY4UbHtJak2RQisahvSShe79mgjO19Lh4O4T3BlbkFJ6CWb8mh7gOQvBYc5qWC83CP8xNyv0bq9vXWT8GHTEA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLEARML_WEB_HOST=http://webserver:8080/\n",
      "env: CLEARML_API_HOST=http://apiserver:8008\n",
      "env: CLEARML_FILES_HOST=http://fileserver:8081\n",
      "env: CLEARML_API_ACCESS_KEY=WOLAKOQGPNE6UYI1O3MGC57ZLNX7IN\n",
      "env: CLEARML_API_SECRET_KEY=JTD4z46ycq_VRIlG60rRPAdE2kkjd1O4jtHWTvhVmiRKvg82MKV-YfoeNIgX12bNuIQ\n"
     ]
    }
   ],
   "source": [
    "%env CLEARML_WEB_HOST=http://webserver:8080/\n",
    "%env CLEARML_API_HOST=http://apiserver:8008\n",
    "%env CLEARML_FILES_HOST=http://fileserver:8081\n",
    "%env CLEARML_API_ACCESS_KEY=WOLAKOQGPNE6UYI1O3MGC57ZLNX7IN\n",
    "%env CLEARML_API_SECRET_KEY=JTD4z46ycq_VRIlG60rRPAdE2kkjd1O4jtHWTvhVmiRKvg82MKV-YfoeNIgX12bNuIQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ChatGPT function to create a question-answer pair\n",
    "\n",
    "This question-answer pair will be part of the dataset for finetuning at a later milestone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_chatgpt_question_answer_pair(chunk_text):\n",
    "    \n",
    "    #Define headers for api call\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {CHATGPT_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    #Define payload with a prompt template to get a question-answer pair from a ROS2 document chunk\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"You are a helpful assistant generating question-answer pairs for text data.\"\n",
    "                    f\"Generate a question-answer pair for the 3 sets of context related to ROS2 given by the user.\"\n",
    "                    f\"Format your answer in this way:\\nQuestion:<question>\\nAnswer:\\n<answer>\"\n",
    "                    f\"Ensure your question corresponds to the main elements of the context given and your answers are elaborate and informative for developers.\"\n",
    "                    f\"Include technical questions regarding code if present and format your answer to highlight the technical concepts in a detailed manner.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"The context is below: \\n\"\n",
    "                    f\"{chunk_text}\"\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    #Get the response from chatgpt\n",
    "    response = requests.post(CHATGPT_API_URL, headers=headers, json=payload)\n",
    "    \n",
    "    #Parse the question and answer from the chatgpt response\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Extract the generated questions and answers\n",
    "        message_content = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "        qa_pairs = {\"question\": \"\", \"answer\": \"\", \"context\": chunk_text}\n",
    "        \n",
    "        question = True\n",
    "        for line in message_content.split(\"\\n\"):\n",
    "            if line.strip():  # Skip empty lines\n",
    "                if str(line).startswith(\"Question:\"):\n",
    "                    question = True\n",
    "                    line = line.split(\"Question:\")[1]\n",
    "                elif str(line).startswith(\"Answer:\"):\n",
    "                    question = False\n",
    "                    line = line.split(\"Answer:\")[1]\n",
    "                \n",
    "                if question:\n",
    "                    qa_pairs[\"question\"] += line\n",
    "                    qa_pairs[\"question\"] += \"\\n\"\n",
    "                else:\n",
    "                    qa_pairs[\"answer\"] += line\n",
    "                    qa_pairs[\"answer\"] += \"\\n\"\n",
    "          \n",
    "        return qa_pairs\n",
    "    else:\n",
    "        print(f\"Error with ChatGPT API: {response.status_code}, {response.text}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to clean the data obtained from the mongo db after web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(documents, collection_name):\n",
    "    cleaned_docs = []\n",
    "    \n",
    "    #Standardize data key names\n",
    "    for doc in documents:\n",
    "        if collection_name in [\"LinkedIn\", \"Medium\"]:\n",
    "            cleaned_docs.append({\n",
    "                \"name\": doc[\"name\"],\n",
    "                \"url\": doc[\"url\"],\n",
    "                \"data\": doc[\"data\"]\n",
    "            })\n",
    "        elif collection_name == \"YouTube\":\n",
    "            cleaned_docs.append({\n",
    "                \"name\": doc[\"video_title\"],\n",
    "                \"url\": doc[\"url\"],\n",
    "                \"data\": doc[\"transcript\"]\n",
    "            })\n",
    "        elif collection_name == \"GitHub\":\n",
    "            cleaned_docs.append({\n",
    "                \"name\": doc[\"repo\"],\n",
    "                \"url\": doc[\"url\"],\n",
    "                \"data\": doc[\"data\"]\n",
    "            })\n",
    "    return cleaned_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to get MiniLM embedding model for featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embedding_model(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the embedding model, the below function creates vector embeddings for a batch of texts using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts, tokenizer, model, device=\"cuda:0\"):\n",
    "    \n",
    "    #Initialize MiniLM tokenizer\n",
    "    tokens = tokenizer(\n",
    "        texts, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    #Generate vector embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function to chunk the documents into smaller parts\n",
    "\n",
    "Using `RecursiveCharacterTextSplitter` allows chunking based on logical separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(doc, collection_name, chunk_size=1000):\n",
    "    data = doc[\"data\"]\n",
    "    url = doc.get(\"url\", None)\n",
    "\n",
    "    # Initialize LangChain's RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, # Max tokens/characters per chunk\n",
    "        chunk_overlap=50, # Overlap between chunks to preserve context\n",
    "        length_function=len # Use character length to measure\n",
    "    )\n",
    "\n",
    "    # Split the text into chunks\n",
    "    split_chunks = text_splitter.split_text(data)\n",
    "\n",
    "    # Create chunk metadata and store in chunks list\n",
    "    chunks = [\n",
    "        {\"content\": chunk, \"metadata\": {\"url\": url, \"collection\": collection_name, \"title\": doc[\"name\"]}}\n",
    "        for chunk in split_chunks\n",
    "    ]\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to upsert embedded vectors in qdrant DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below initializes the qdrant db for storing the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_qdrant(db_path=\"http://qdrant:6333\", collection_name=\"ros2_docs\", vector_size=512):\n",
    "    client = QdrantClient(db_path, timeout=15)\n",
    "\n",
    "    #Ensure collection exists with the correct configuration\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config={\"size\": vector_size, \"distance\": \"Cosine\"}\n",
    "    )\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_upsert(chunks, qdrant_client: QdrantClient, tokenizer, model, collection_name, start_id, device=\"cuda:0\"):  \n",
    "    \n",
    "    #Process 64 chunks in a batch  \n",
    "    batch_size=64\n",
    "    \n",
    "    for b in tqdm(range(0, len(chunks), batch_size), \"Upserting\", leave=False):\n",
    "        \n",
    "        # Generate embeddings for all chunks in a batch\n",
    "        batch_embeddings = generate_embeddings([chunk[\"content\"] for chunk in chunks[b:b+batch_size]], tokenizer, model, device=device)\n",
    "        \n",
    "        # Convert the embeddings and metadata into the points structure\n",
    "        points = []\n",
    "        for i in range(len(batch_embeddings)):\n",
    "            chunk = chunks[b+i]\n",
    "            payload = {\n",
    "                \"content\": chunk[\"content\"],\n",
    "                \"metadata\": chunk[\"metadata\"]\n",
    "            }\n",
    "            \n",
    "            points.append(\n",
    "                PointStruct(\n",
    "                    id=start_id + b + i,\n",
    "                    vector=batch_embeddings[i].tolist(),\n",
    "                    payload=payload\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "        # Upsert into Qdrant\n",
    "        qdrant_client.upsert(collection_name=collection_name, points=points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2085/522091167.py:5: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer, model = initialize_embedding_model()\n",
    "model.to(device)\n",
    "embedding_dim = model.config.hidden_size\n",
    "qdrant_client = initialize_qdrant(vector_size=embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the featurization pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function parses the chunk to be used as context for question-answer pair generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed_chunk(chunk, i):\n",
    "    title = chunk[\"metadata\"][\"title\"]\n",
    "    content = chunk[\"content\"]\n",
    "    return f\"Context {i} Title: {title}\\nContext content: \\n{content}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=61b4ee3dc14c4ee7bec5b1d0499780a2\n",
      "2024-12-08 00:09:02,385 - clearml.Repository Detection - WARNING - Could not read Jupyter Notebook: No module named 'nbconvert'\n",
      "2024-12-08 00:09:02,387 - clearml.Repository Detection - WARNING - Please install nbconvert using \"pip install nbconvert\"\n",
      "ClearML results page: http://webserver:8080/projects/e54dc86522f8446889c50edbdf493bef/experiments/61b4ee3dc14c4ee7bec5b1d0499780a2/output/log\n",
      "CLEARML-SERVER new package available: UPGRADE to v1.17.0 is recommended!\n",
      "Release Notes:\n",
      "### New Features \n",
      "- New ClearML Model dashboard: View all live model endpoints in a single location, complete with real time metrics reporting.\n",
      "- New UI pipeline run table comparative view: compare plots and scalars of selected pipeline runs\n",
      "- Improve services agent behavior: If no credentials are specified, agent uses default credentials ([ClearML Server GitHub issue #140](https://github.com/allegroai/clearml-server/issues/140))\n",
      "- Add UI re-enqueue of failed tasks\n",
      "- Add UI experiment scalar results table view\n",
      "- Add \"Block running user's scripts in the browser\" UI setting option for added security\n",
      "- Add UI \"Reset\" to set task installed packages to originally recorded values \n",
      "- Add UI edit of default Project default output destination\n",
      "\n",
      "### Bug Fixes\n",
      "- Fix broken download links to artifacts stored in Azure ([ClearML Server GitHub issue #247](https://github.com/allegroai/clearml-server/issues/247))\n",
      "- Fix Cross-site Scripting (XSS) vulnerability (CWE-79)\n",
      "- Fix UI experiment textual comparison diff showing contextual diffs as different contents ([ClearML GitHub issue #646](https://github.com/allegroai/clearml/issues/646))\n",
      "- Fix UI experiment comparison does not overlay box plots and histograms ([ClearML GitHub issue #1298](https://github.com/allegroai/clearml/issues/1298))\n",
      "- Fix UI plots display “Iteration 0” when it is the only reported iteration ([ClearML GitHub issue #1267](https://github.com/allegroai/clearml/issues/1267))\n",
      "- Fix scalar series starting with angle bracket (`<`) causes UI scalar plot legend to display raw html  ([ClearML GitHub issue #1292](https://github.com/allegroai/clearml/issues/1292))\n",
      "- Fix UI scalar plot not displayed if metric name includes a slash surrounded by spaces (` / `)\n",
      "- Fix Model API calls fail when its creating task has been deleted ([ClearML GitHub issue #1299](https://github.com/allegroai/clearml/issues/1299))\n",
      "- Fix UI experiment debug samples disappear while task is running ([ClearMLGitHub issue #1259](https://github.com/allegroai/clearml/issues/1259))\n",
      "- Fix UI pipeline DAG display to be \"bottom-up\" so all final steps appear in the bottom of the diagram ([ClearML Web GitHub PR #86](https://github.com/allegroai/clearml-web/pull/86))\n",
      "- Fix invalid mongodb connection string if `CLEARML_MONGODB_SERVICE_CONNECTION_STRING` is specified ([ClearML Server GitHub issue #252](https://github.com/allegroai/clearml-server/issues/252))\n",
      "- Fix auto-refresh modifies UI experiment debug sample view ([ClearML GitHub issue #1529](https://github.com/allegroai/clearml/issues/1529))\n",
      "- Fix UI image plot retrieval from fileserver missing authentication cookie ([ClearML GitHub issue #1331](https://github.com/allegroai/clearml/issues/1331))\n",
      "- Fix pipeline run version not set when re-executed via the UI\n",
      "- Fix metric and hyperparameter group string not searchable in UI table \n",
      "- Fix UI model hidden plot selection does not persist upon refresh\n",
      "- Fix deleting large number of experiments via UI not working properly\n",
      "- Fix experiment name legend not displayed in UI experiment plot comparison \n",
      "- Fix archiving pipeline run does not abort pipeline step tasks\n",
      "- Fix \"Restore\" and \"Delete\" buttons missing from UI experiment archive action bar\n",
      "- Fix UI experiment debug sample viewer sometimes displays incorrect sample\n",
      "- Fix UI Settings' Configuration section does not display indication that the number of credentials limit has been reached\n",
      "- Fix Hydra parameters not displaying correctly in UI experiment comparison parallel coordinates plot\n",
      "- Fix UI Reports image upload not working\n",
      "- Fix iteration number not displayed in titles of confusion matrix and table plots in UI experiment comparison\n",
      "- Fix \"Create new\" option not displaying when inputting project name in UI Report creation modal\n",
      "- Fix UI experiment requires Git information when repository isn't set\n",
      "- Fix moving an enqueued experiment to a new UI queue results in error \n",
      "- Fix modifying UI experiment input models does not work\n",
      "2024-12-08 00:09:03,257 - clearml.Task - INFO - Storing jupyter notebook directly as code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collection:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collection -  LinkedIn\n",
      "Obtained 100 documents from MongoDB\n",
      "Completed cleaning all documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 628 chunks from all the documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collection:  25%|██▌       | 1/4 [00:32<01:38, 32.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed data insertion of LinkedIn chunks into vectorDB\n",
      "Processing collection -  GitHub\n",
      "Obtained 806 documents from MongoDB\n",
      "Completed cleaning all documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 7780 chunks from all the documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collection:  50%|█████     | 2/4 [10:18<11:56, 358.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed data insertion of GitHub chunks into vectorDB\n",
      "Processing collection -  YouTube\n",
      "Obtained 51 documents from MongoDB\n",
      "Completed cleaning all documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1177 chunks from all the documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collection:  75%|███████▌  | 3/4 [11:17<03:41, 221.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed data insertion of YouTube chunks into vectorDB\n",
      "Processing collection -  Medium\n",
      "Obtained 60 documents from MongoDB\n",
      "Completed cleaning all documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 667 chunks from all the documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collection: 100%|██████████| 4/4 [12:00<00:00, 180.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed data insertion of Medium chunks into vectorDB\n",
      "Creating Question-Answer Dataset\n"
     ]
    }
   ],
   "source": [
    "# Collections to Process\n",
    "collections = [\"LinkedIn\", \"GitHub\", \"YouTube\", \"Medium\"]\n",
    "chunk_log = {}\n",
    "\n",
    "#Vector DB id\n",
    "start_id = 0\n",
    "\n",
    "task = Task.init(project_name=\"ROS2 RAG System\", task_name=f\"Featurization\", task_type=Task.TaskTypes.data_processing)\n",
    "logger = task.get_logger()\n",
    "\n",
    "for collection_name in tqdm(collections, \"Collection\"):\n",
    "    logger.report_text(f\"Processing collection -  {collection_name}\")\n",
    "    \n",
    "    #Collection for obtaining scraped content\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    #Collection to store Question-Answer pairs\n",
    "    fine_tuning_col = db[\"QuestionAnswer\"]\n",
    "    \n",
    "    #Obtain all documents in collection\n",
    "    documents = list(collection.find({}))\n",
    "    logger.report_text(f\"Obtained {len(documents)} documents from MongoDB\")\n",
    "\n",
    "    # Clean Data\n",
    "    cleaned_docs = clean_data(documents, collection_name)\n",
    "    logger.report_text(\"Completed cleaning all documents\")\n",
    "\n",
    "    # Chunk Data\n",
    "    all_chunks = []\n",
    "    for doc in tqdm(cleaned_docs, \"Documents\", leave=False):\n",
    "        chunks = chunk_data(doc, collection_name)\n",
    "        \n",
    "        # If enough chunks present, generate a question answer pair using chatgpt\n",
    "        # if len(chunks) >= 3:\n",
    "        #     random_elements = np.random.choice(chunks, size=3, replace=False)\n",
    "        #     context = \"\\n\\n\\n\".join([get_parsed_chunk(ele, i+1) for i, ele in enumerate(random_elements)])\n",
    "        #     qa_pair = call_chatgpt_question_answer_pair(context)\n",
    "        #     qa_pair[\"collection\"] = collection_name\n",
    "        #     fine_tuning_col.insert_one(qa_pair)\n",
    "            \n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    logger.report_text(f\"Created {len(all_chunks)} chunks from all the documents\")\n",
    "    chunk_log[collection_name] = len(all_chunks)\n",
    "\n",
    "    # Embed and Store in Vector DB\n",
    "    embed_and_upsert(all_chunks, qdrant_client, tokenizer, model, \"ros2_docs\", start_id, device)\n",
    "    logger.report_text(f\"Completed data insertion of {collection_name} chunks into vectorDB\")\n",
    "    \n",
    "    start_id += len(all_chunks)\n",
    "\n",
    "logger.report_text(\"Creating Question-Answer Dataset\")\n",
    "\n",
    "#Fetch all question answer pairs\n",
    "# collection = db[\"QuestionAnswer\"]\n",
    "# data = list(collection.find({}))\n",
    "\n",
    "# formatted_data = []\n",
    "\n",
    "# for record in data:\n",
    "#     if 'question' not in record:\n",
    "#         print(\"Incomplete Record. Skipping\")\n",
    "#         continue\n",
    "    \n",
    "#     #Get question, answer and context from each record\n",
    "#     question = record[\"question\"]\n",
    "#     context = record[\"context\"]\n",
    "#     answer = record[\"answer\"]\n",
    "\n",
    "#     #Create the prompt and gpt response expected\n",
    "#     prompt = f\"This is the document context:\\n{context}\\n\\n\\nThis is the user prompt: {question}\\n\\nUsing the given document context pertaining to ROS2, answer the question prompted by the user.\"\n",
    "#     completion = f\"{answer.strip()}\"\n",
    "\n",
    "#     #Store the data in a format required for finetuning\n",
    "#     formatted_data.append({\"conversations\": [{\"from\": \"human\", \"value\": prompt}, {\"from\": \"gpt\", \"value\": completion}]})\n",
    "\n",
    "# # Save as JSON\n",
    "# with open(\"formatted_dataset.json\", \"w\") as f:\n",
    "#     json.dump(formatted_data, f, indent=4)\n",
    "\n",
    "task.upload_artifact(\"Finetuning_QA\", artifact_object='formatted_dataset.json')\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Chunks Created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 10252 chunks in vector db!\n"
     ]
    }
   ],
   "source": [
    "total_chunks = sum([chunk_log[source] for source in chunk_log])\n",
    "print(f\"Inserted {total_chunks} chunks in vector db!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-Answer Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"formatted_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Question Answer Pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 951 pairs of question and answers\n"
     ]
    }
   ],
   "source": [
    "pairs_count = len(data)\n",
    "print(f\"Created {pairs_count} pairs of question and answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset Question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the document context:\n",
      "Context 1 Title: moveit/moveit2_tutorials\n",
      "Context content: \n",
      "In Shell 3:\n",
      "\n",
      "    \n",
      "    \n",
      "    ros2 launch moveit2_tutorials perception_pipeline_demo.launch.py\n",
      "    \n",
      "\n",
      "In Shell 4:\n",
      "\n",
      "    \n",
      "    \n",
      "    ros2 bag play -r 5 <your_bag_file> --loop\n",
      "    \n",
      "\n",
      ":codedir:`perception_pipeline_demo.launch.py\n",
      "<examples/perception_pipeline/launch/perception_pipeline_demo.launch.py>` is\n",
      "similar to :codedir:`demo.launch.py\n",
      "</doc/tutorials/quickstart_in_rviz/launch/demo.launch.py>` inside MoveIt\n",
      "Quickstart in RViz except a couple of details. For\n",
      "`perception_pipeline_demo.launch.py`, following lines is added to\n",
      "`moveit_config`.\n",
      "\n",
      "You can find these additional lines in line 51, 52 and 53 inside\n",
      "`perception_pipeline_demo.launch.py`:\n",
      "\n",
      "    \n",
      "    \n",
      "    .sensors_3d(file_path = os.path.join(\n",
      "                get_package_share_directory(\"moveit2_tutorials\"),\n",
      "                \"config/sensors_3d.yaml\"))\n",
      "    \n",
      "\n",
      "Finally, all demo codes can be found in :codedir:`perception_pipeline's\n",
      "directory <examples/perception_pipeline>` on Github.\n",
      "\n",
      "# Project name not set\n",
      "\n",
      "### Navigation\n",
      "\n",
      "\n",
      "Context 2 Title: moveit/moveit2_tutorials\n",
      "Context content: \n",
      "sensors_3d.yaml:\n",
      "\n",
      "    \n",
      "    \n",
      "    sensors:\n",
      "      - camera_1_pointcloud\n",
      "      - camera_2_depth_image\n",
      "    camera_1_pointcloud:\n",
      "        sensor_plugin: occupancy_map_monitor/PointCloudOctomapUpdater\n",
      "        point_cloud_topic: /camera_1/points\n",
      "        max_range: 5.0\n",
      "        point_subsample: 1\n",
      "        padding_offset: 0.1\n",
      "        padding_scale: 1.0\n",
      "        max_update_rate: 1.0\n",
      "        filtered_cloud_topic: /camera_1/filtered_points\n",
      "    camera_2_depth_image:\n",
      "        sensor_plugin: occupancy_map_monitor/DepthImageOctomapUpdater\n",
      "        image_topic: /camera_2/depth/image_raw\n",
      "        queue_size: 5\n",
      "        near_clipping_plane_distance: 0.3\n",
      "        far_clipping_plane_distance: 5.0\n",
      "        shadow_threshold: 0.2\n",
      "        padding_scale: 1.0\n",
      "        max_update_rate: 1.0\n",
      "        filtered_cloud_topic: /camera_2/filtered_points\n",
      "    \n",
      "\n",
      "### Configurations for Point Cloud¶\n",
      "\n",
      "The general parameters are:\n",
      "\n",
      "  * _sensor_plugin_ : The name of the plugin that we are using.\n",
      "\n",
      "\n",
      "Context 3 Title: moveit/moveit2_tutorials\n",
      "Context content: \n",
      "By the way, you can also use :codedir:`this rviz file\n",
      "<examples/perception_pipeline/rviz2/depth_camera_environment.rviz>` on Github\n",
      "to visualize poincloud topics in rviz.\n",
      "\n",
      "In next step, we will use the recorded bag file to create an octomap.\n",
      "\n",
      "## Configuration For 3D Sensors¶\n",
      "\n",
      "MoveIt uses an octree-based framework to represent the world around it. The\n",
      "_Octomap_ parameters above are configuration parameters for this\n",
      "representation:\n",
      "\n",
      "    \n",
      "\n",
      "  * _octomap_frame_ : specifies the coordinate frame in which this representation will be stored. If you are working with a mobile robot, this frame should be a fixed frame in the world. We can set this frame for plugin by frame_id field of ros messages like pointcloud and image topic.\n",
      "\n",
      "  * _octomap_resolution_ : specifies the resolution at which this representation is maintained (in meters).\n",
      "\n",
      "  * _max_range_ : specifies the maximum range value to be applied for any sensor input to this node.\n",
      "\n",
      "\n",
      "This is the user prompt:  What are the key components and configurations involved in setting up a perception pipeline using MoveIt2, as depicted in the provided contexts?\n",
      "\n",
      "\n",
      "Using the given document context pertaining to ROS2, answer the question prompted by the user.\n"
     ]
    }
   ],
   "source": [
    "sam = np.random.choice(data, 1)[0]\n",
    "print(sam[\"conversations\"][0][\"value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The setup for a perception pipeline using MoveIt2 involves several key components and configurations that are crucial for successfully integrating and visualizing sensor data. \n",
      "1. **Launching the Perception Pipeline**: The first step involves launching the perception pipeline demo using the command:\n",
      "   ```\n",
      "   ros2 launch moveit2_tutorials perception_pipeline_demo.launch.py\n",
      "   ```\n",
      "   This command initializes the necessary nodes and parameters defined in the `perception_pipeline_demo.launch.py` file, which is a specialized launch file for the perception pipeline.\n",
      "2. **Playing Recorded Bag Files**: In conjunction with launching the perception pipeline, you can play back recorded sensor data using:\n",
      "   ```\n",
      "   ros2 bag play -r 5 <your_bag_file> --loop\n",
      "   ```\n",
      "   This command allows you to replay data at a rate of 5x, which is useful for testing and visualizing the sensor inputs in real time.\n",
      "3. **Sensor Configuration**: The configuration for sensors is specified in the `sensors_3d.yaml` file. This configuration includes:\n",
      "   - **Sensor List**: Defines the types of sensors being used, such as `camera_1_pointcloud` and `camera_2_depth_image`.\n",
      "   - **Sensor Plugins**: Each sensor has an associated plugin, for example:\n",
      "     - `occupancy_map_monitor/PointCloudOctomapUpdater` for point cloud data.\n",
      "     - `occupancy_map_monitor/DepthImageOctomapUpdater` for depth image data.\n",
      "   - **Parameters**: Several parameters are defined for each sensor, such as:\n",
      "     - `max_range`: The maximum distance the sensor can detect (e.g., 5.0 meters).\n",
      "     - `filtered_cloud_topic`: The topic where the filtered point cloud data will be published.\n",
      "4. **Visualization in RViz**: To visualize the point cloud topics, you can use a pre-configured RViz file located at:\n",
      "   ```\n",
      "   examples/perception_pipeline/rviz2/depth_camera_environment.rviz\n",
      "   ```\n",
      "   This allows users to visually inspect the data being processed by the pipeline.\n",
      "5. **Octomap Configuration**: MoveIt utilizes an octree-based framework for world representation. Key parameters for the Octomap include:\n",
      "   - `_octomap_frame_`: Defines the fixed coordinate frame for the octomap representation, which is important for mobile robots to maintain a consistent reference.\n",
      "   - `_octomap_resolution_`: This parameter dictates the granularity of the octomap (in meters).\n",
      "   - `_max_range_`: This specifies the maximum range for sensor inputs, ensuring that only relevant data is processed.\n",
      "By understanding and configuring these components, developers can effectively set up a perception pipeline that leverages sensor data for robotic applications.\n"
     ]
    }
   ],
   "source": [
    "print(sam[\"conversations\"][1][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
